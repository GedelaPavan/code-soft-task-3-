# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# data manipulation
import pandas as pd
import numpy as np

# visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_theme(style='whitegrid', palette='viridis')

# preprocessing
from sklearn.preprocessing import StandardScaler, LabelEncoder

# deep learning
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

# path
import time
import os
for dirname, _, filenames in os.walk('/content/archive (6).zip'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/archive (6).zip')

df.drop(columns = ['RowNumber', "CustomerId","Surname"], inplace=True, axis=1)

df.shape

pip install skimpy

from skimpy import skim

skim(df)

def summary(df):
  summary_df = pd.DataFrame(df.dtypes, columns=['dtypes'])
  summary_df['count'] = df.count().values
  summary_df['unique'] = df.nunique().values
  return summary_df
summary(df).style.background_gradient(cmap='Purples')

df.duplicated().sum()

def cleaned(df):
  le = LabelEncoder()
  df['Geography'] = le.fit_transform(df['Geography'])
  df['Gender'] = le.fit_transform(df['Gender'])
  num_features = ['CreditScore','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']
  sc = StandardScaler()
  df[num_features] = sc.fit_transform(df[num_features])
  return df

df0 = df.copy()

df_cleaned = cleaned(df)

df_cleaned.head()

X = df_cleaned.drop('Exited', axis=1)
y = df_cleaned['Exited']

X.head()

y.head()

plt.figure(figsize = [15,8])
fig = sns.heatmap(df_cleaned.corr(), cmap='hot_r',
                 annot=True, linecolor='black',linewidths=0.01, annot_kws={"fontsize":12}, fmt="0.2f")

top, bottom = fig.get_ylim()
fig.set_ylim(top+0.1,bottom-0.1)

left, right = fig.get_xlim()
fig.set_xlim(left-0.1,right+0.1)

plt.yticks(fontsize=13,rotation=0)
plt.xticks(fontsize=13,rotation=90)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state =42)

tf.random.set_seed(42)
epoch = 130

# Early stopping callback configuration
early_stopping = EarlyStopping(
    monitor='val_auc_metric',   # Metric to monitor for improvement
    min_delta=0.0001,           # Minimum change to qualify as an improvement
    patience=5,                 # Number of epochs to wait for improvement before stopping
    start_from_epoch=20,        # Start monitoring after this many epochs
    restore_best_weights=True,   # Restore model weights from the epoch with the best value of the monitored metric
    mode = 'max'
)

model = models.Sequential([
    layers.BatchNormalization(input_shape=[X.shape[1]]),
    layers.Dense(128, activation='leaky_relu', use_bias=True),
    layers.Dense(452, activation='relu', use_bias=True),
    layers.Dense(362, activation='relu', use_bias=True),
    layers.Dropout(0.4),
    layers.Dense(256, activation='relu', use_bias=True),
    layers.Dense(387, activation='leaky_relu', use_bias=True),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu', use_bias=True),
    layers.Dense(452, activation='relu', use_bias=True),
    layers.Dense(362, activation='relu', use_bias=True),
    layers.Dropout(0.4),
    layers.Dense(256, activation='leaky_relu', use_bias=True),
    layers.Dense(128, activation='relu', use_bias=True),
    layers.Dense(64, activation='leaky_relu', use_bias=True),
    layers.Dense(32, activation='relu', use_bias=True),
    layers.Dense(1, activation='sigmoid', use_bias=True)
])

model.compile(
    optimizer = 'adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy']
)

model.summary()

def model_evaluation_plot(history):
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    plt.plot(history.history['binary_accuracy'], label='Training Accuracy', color='orange')

    plt.title('Training Loss and Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Metrics')
    plt.legend()
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# history = model.fit(
#     X_train, y_train,
#     epochs=epoch,
#     validation_data=(X_test, y_test),
#     callbacks=[early_stopping]
# )



model_evaluation_plot(history)

loss,accuracy=model.evaluate(X_test,y_test)
print(accuracy,loss)

loss, accuracy=model.evaluate(X_train, y_train)
print(accuracy, loss)